# 함수 (Function)

* 함수를 사용하는 이유
    * 코드를 간결하게 하기 위함
    * 코드가 길어지더라도 가독성을 높이기 위함
* 형식 인자 (매개 변수, parameter)
    * 함수를 정의할 때 나열하는 변수
* 실질 인자 (전달 인자, argument)
    * 함수를 사용할 때 실제로 전달되는 값


[참고 자료](https://wayhome25.github.io/etc/2017/12/31/parameter-argument/)

 * 참조에 의한 호출 (pass by reference)
    * 함수를 호출한 쪽에서도 실지 인자 변수의 값이 변함
    * ```string```, ```numeric```  등의 데이티형 (data type)은 이와 같은 방식 
    * 함수에서 **외부 메모리 공간 참조**할 때 사용
 * 값에 의한 호출 (pass by value)
    * 형식 인자에 새로운 값이 할당되어도 실질 인자 변수에 영향 없음
    * ```return```으로 호촐해야 참조에 의한 호출처럼 사용할 수 있음
    * 위와 같은 효과를 위해 ```global``` 명령어를 사용할 수도 있음
        * 단 ```global``` 명령어를 남용할 경우 최종으로 입력된 값으로 값이 바뀌므로 유의 
    * ```list``` 등의 데이티형 (data type)은 이와 같은 방식   
        
        ```
        # 가능
        a = 1
        def my_function():
            print(a)
        
        # local variable 선언 전에 사용했다는 오류 발생
        a = 1
        def my_function():
            a = a + 1
        ``` 
* 캡슐화
    * 함수나 클래스 안에서 정의된 변수는 내부에서만 사용 가능
    * ```import```되었을 때에는 독특하게 동작   
        
        ```
        # A.py
        import B

        # 호출 시 obj 출력 가능
        # 호출 시 마다 obj 다시 선언
        my_function() 

        # print(obj) 는 불가능

        # B.py

        global obj = kkk

        def my_function():
            print(obj)
        ```
* ```JavaScript```, ```python```은 ```for```, ```if```문 안에서 선언된 변수 global 함수
    * ```for``` 문 내 정의된 변수에 apply해 바로 적용해볼 수 있음     
* ```Java```의 경우 함수 내부뿐만 아니라 ```for```, ```if```문 안에서 선언된 변수 지역 변수 함수
* python 3.7 ```->``` annotation에 해당

# Python 접근 제한자와 마술 함수
* python은 변수 또는 함수 앞에 ```__```을 붙임으로써 ```private``` 선언
    * ```_ClassName__variableName``` 형태로 바뀌어서 ```__variableName```으로는 접근 불가하나 위의 이름 사용해야 함 
*
# spark
* 코어가 많아야 함
* 메모리 위에서 동작
    * 하드웨어 IO를 최소화
* RDD
    * 분산되어 여러 개의 조각
    * 이가 기본이 됨
    * 오버헤드가 큼
        * GC (Garbage Collection) 비용이 비쌈
    * readonly
        * lazy loading으로 논리적 구조만 짜둔 후 ```action```(```count```, ```collect``` 등)이 발생하면 한 번에 계산한 뒤 값을 반환
        * 단 반환된 값은 객체 형태가 아니기 때문에 출력하거나 쓰거나 해야 함
        * ```pandas```의 ```data frame```은 메모리 점유율이 변동하는 게 단점 
    * 데이터 무결성 확인 등이 용이
    * 중간에 메모리가 부족해질 경우 다른 노드에서의 처리 등으로 해결 가능
    * 메모리에 결국 올려야 한다는 점이 단점이나 Map Reduce로 극복 가능
    * subset 지원 안 되어 train 데이터셋을 별도로 필요하다는 점에서 다소 불편
    * Hadoop은 데이터가 나누어져 있어서 Graph 작업이 불가능하지만 Spark 가능  
* Spark data frame
    * RDD 기반으로 만들어 실체가 있는 것은 아님
    * Spark 제공 메소드는 빠르게 동작
        * 단 사용자 정의 메소드는 지원이 안 됨
        * ```df.rdd.map```으로 사용해야 하지만 느리다고 함